{"cells": [{"metadata": {}, "cell_type": "markdown", "source": "# 1. Prepare Environment"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# import the necessary module \nimport time\nfrom pyspark.ml.feature import Word2Vec,HashingTF,IDF,Tokenizer\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import StructType, StructField, StringType,IntegerType, FloatType\nfrom pyspark.sql import SparkSession\nimport tensorflow_hub as hub\nimport tensorflow as tf\nimport numpy as np\nimport nltk\nimport re\ntokenizer = nltk.data.load('/home/hadoop/nltk_data/tokenizers/punkt/english.pickle')", "execution_count": 61, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "47bffca2e4684611bc6f3a0ca1153005"}}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "spark = SparkSession \\\n    .builder \\\n    .appName(\"Assignment2\") \\\n    .getOrCreate()", "execution_count": 62, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "b57dee4b022e439cbf1c82eba1046913"}}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "# 2. ml_utils"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# design necessary methods that will be used in Stage1, Stage2, Stage3, Stage4\nstore_route = \"s3://5349a2/\"\nmodule_url = \"https://tfhub.dev/google/universal-sentence-encoder/2\" #@param [\"https://tfhub.dev/google/universal-sentence-encoder/2\", \"https://tfhub.dev/google/universal-sentence-encoder-large/3\"]\nembed = hub.Module(module_url)\ndef getmediumnumber(dataset,length,colname):\n    \"\"\"\n    get the median number\n    \"\"\"\n    median_num_index = []\n    median_num = 0\n    if length%2 ==0:\n        median_num_index = [length/2-1,length/2]\n    else:\n        median_num_index = [length/2]\n    for i in median_num_index:\n        median_num += dataset[int(i)][colname]\n    median_num = median_num/float(len(median_num_index))*(-1)\n    return median_num\n\ndef sentence_token_nltk(string):\n    \"\"\"\n    use the method from nltk to segment this paragraph to sentences\n    \"\"\"\n    sent_tokenize_list = tokenizer.tokenize(string)\n    return sent_tokenize_list\ndef sentence_len(str_centence):\n    \"\"\"\n    calculate the amount of sentences of the review body\n    \"\"\"    \n    if str_centence == None:\n        return 0\n    str_centence = str(str_centence)\n    list_ret = tokenizer.tokenize(str_centence)\n    return len(list_ret)\ndef dividesentences(record):\n    \"\"\"\n    segment this paragraph to sentences\n    \"\"\"\n    sentences = sentence_token_nltk(record[-1])\n    reviewid = record[0]\n    sentencelist = []\n    for sentence in sentences:\n        sentencelist.append((reviewid,sentence))\n    return sentencelist\ndef review_embed(sentence_list):\n    \"\"\"\n    use google methods embedding sentence\n    \"\"\"   \n    with tf.Session() as session:\n        session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n        message_embeddings = session.run(embed(sentence_list))\n    return message_embeddings\ndef googleembeddingmethod(text_rdd):\n    \n    review_embedding = text_rdd.mapPartitions(review_embed).collect()\n    return review_embedding \n# def cosdistance(A,B):\n#     num = float(np.dot(A * B.T)) \n#     denom = np.linalg.norm(A) * np.linalg.norm(B)  \n#     cos = num / denom \n#     return (1-cos)\n\n", "execution_count": 63, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "c2f8331627454c55bfbc1a8b6e6e5282"}}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "# 3. Getting The necessary data  "}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# rating_data_route = \"s3://5349a2/test_Music.tsv\"\nrating_data_route = \"s3://amazon-reviews-pds/tsv/amazon_reviews_us_Music_v1_00.tsv.gz\"\nmusic_data_df = spark.read.csv(rating_data_route,header=True,sep='\\t')\\\n            .select(\"customer_id\",\"product_id\",\"star_rating\",\"review_id\",\"review_body\").na.drop()\n\n# write the result to file \nmusic_data_df.write.csv(store_route+'music_data_df.csv',header = True,mode = \"overwrite\")\n\n\n\n\n", "execution_count": 64, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "417efbe54d164a8c9a5272e1a756f9a0"}}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "# Stage 1. overall review"}, {"metadata": {}, "cell_type": "markdown", "source": "Stage1 --- (a)  total number of reviews, user, and product"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "t1 = time.time()\n# read the selected music data from s3 \nmusic_data_df = spark.read.csv(store_route+'music_data_df.csv', header=True, inferSchema=True).cache()\n\n#total number of reviews\ntotal_number_of_reviews= music_data_df.select(\"review_id\").count()\n#total number of unique user\ntotal_number_unique_user = music_data_df.select(\"customer_id\").distinct().count()\n#total number of unique product\ntotal_number_unique_product = music_data_df.select(\"product_id\").distinct().count()\n\n\n# print(total_number_of_reviews)\n# print(total_number_unique_user)\n# print(total_number_unique_product)", "execution_count": 97, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "0b814a1a9e9140779cc021d58cd656ae"}}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "Stage1 --- (b)  Top 10, largest number and medium number of reviews created by costomer"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "\nnumber_of_reviews_by_user_pre = music_data_df.select(\"customer_id\").groupby(\"customer_id\").count()\nnumber_of_reviews_by_user=number_of_reviews_by_user_pre.select(number_of_reviews_by_user_pre[\"customer_id\"],number_of_reviews_by_user_pre[\"count\"]*(-1))\nnumber_of_reviews_by_user_list = number_of_reviews_by_user.orderBy(\"(count * -1)\").limit(int(total_number_unique_user/2+1)).collect()\n\n#median number of customer\nmedian_num_user = getmediumnumber(number_of_reviews_by_user_list,total_number_unique_user,\"(count * -1)\")\n#top 10 number of reviews by user\ntop10_number_of_reviews_by_user =[number_of_reviews_by_user_list[row_index][\"customer_id\"] for row_index in range(10)]\n#largest number of reviews by user\nlargest_number_of_reviews_by_user = number_of_reviews_by_user_list[0][\"(count * -1)\"]*(-1)\n\n# print(top10_number_of_reviews_by_user)\n# print(largest_number_of_reviews_by_user)\n# print(median_num_user)\n\n", "execution_count": 98, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "e12948c41d974d94816471c925dc17b9"}}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "Stage1 --- (c)  Top 10, largest number and medium number of reviews created by costomer"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "number_of_reviews_for_single_product_pre = music_data_df.select(\"product_id\").groupby(\"product_id\").count()\nnumber_of_reviews_for_single_product=number_of_reviews_for_single_product_pre.select(number_of_reviews_for_single_product_pre[\"product_id\"],number_of_reviews_for_single_product_pre[\"count\"]*(-1)).orderBy(\"(count * -1)\").limit(int(total_number_unique_product/2+1))\nnumber_of_reviews_for_single_product_list = number_of_reviews_for_single_product.collect()\n\n#median number of customer\nmedian_num_product = getmediumnumber(number_of_reviews_for_single_product_list,total_number_unique_product,\"(count * -1)\")\n\n#top 10 number of reviews for a product\ntop10_number_of_reviews_for_product =[number_of_reviews_for_single_product_list[row_index][\"product_id\"] for row_index in range(10)]\n#largest number of reviews for a user\nlargest_number_of_reviews_for_product = number_of_reviews_for_single_product_list[0][\"(count * -1)\"]*(-1)\n\n\n# print(top10_number_of_reviews_for_product)\n# print(largest_number_of_reviews_for_product)\n# print(median_num_product)\n\n# write this result to S3 because this filtered dateframe will be used in the later processing\nnumber_of_reviews_for_single_product.limit(10).write.csv(store_route+'number_of_reviews_for_single_product.csv', header = True, mode = \"overwrite\")\nt2 = time.time()\nprint(t2-t1)\n\n\n\n", "execution_count": 99, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "f0fd31c3c1d1424da29b82cc4177037f"}}, "metadata": {}}, {"output_type": "stream", "text": "57.30673003196716", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "number_of_reviews_by_user = music_data_df.select(\"customer_id\").groupby(\"customer_id\").count()\nuserid_over_median = number_of_reviews_by_user.filter(number_of_reviews_by_user[\"count\"]>median_num_user)\n\nnumber_of_reviews_for_single_product = music_data_df.select(\"product_id\",\"customer_id\").groupby(\"product_id\").count()\nproductid_over_median = number_of_reviews_for_single_product.filter(number_of_reviews_for_single_product[\"count\"]>median_num_product)\n\n# write this userid_over_median dataframe to S3 because this filtered dateframe will be used in the later processing\nuserid_over_median.write.csv(store_route+'userid_over_median.csv', header = True, mode = \"overwrite\")\nproductid_over_median.write.csv(store_route+'productid_over_median.csv', header = True, mode = \"overwrite\")", "execution_count": 100, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "200691deb8ea476d86b4b26b6ada6374"}}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "# Stage 2. remove nnwanted rows"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "t1 = time.time()\nmusic_data_df = spark.read.csv(store_route+'music_data_df.csv', header=True, inferSchema=True).cache()\n\n\nuserid_over_median = spark.read.csv(store_route+'userid_over_median.csv', header=True, inferSchema=True)\nproductid_over_median = spark.read.csv(store_route+'productid_over_median.csv', header=True, inferSchema=True)\n\n\ntoDateUDF=udf(sentence_len, IntegerType())\nreview_body_len = music_data_df.withColumn('s_len',toDateUDF('review_body'))\nreview_body_over_2_sentence = review_body_len.filter(review_body_len[\"s_len\"]>=2)\n\n\n", "execution_count": 101, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "73060284cf954b71b524516227f149df"}}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "#use inner join to get the filtered data\nfilter_movie_data = review_body_over_2_sentence.join(productid_over_median,\"product_id\",\"inner\").select(\"customer_id\",\"product_id\",\"star_rating\",\"review_id\",\"review_body\",\"s_len\")\\\n.join(userid_over_median,\"customer_id\",\"inner\").select(\"customer_id\",\"product_id\",\"star_rating\",\"review_id\",\"review_body\",\"s_len\")\n\nfilter_movie_data2 = filter_movie_data.select(\"product_id\",\"review_id\",\"star_rating\",\"review_body\")\n#this data will be used in later processing\nfilter_movie_data2.write.csv(store_route+'filter_movie_data.csv', header = True, mode = \"overwrite\")\n\n", "execution_count": 102, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "b3c03108b03c44ecafd6679124cf47ad"}}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "filter_movie_data_rdd = filter_movie_data.rdd.cache()\n", "execution_count": 103, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "1ffa87537b5b4d6a8a50295956a851a4"}}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "reviews_by_user = filter_movie_data_rdd.map(lambda x:(x[0],x[5])).groupByKey()\nreviews_by_user_sort = reviews_by_user.mapValues(lambda l : sorted(l,key = lambda x : x)[int(len(l)/2)])\nreviews_by_user_sort_10 = reviews_by_user_sort.sortBy(lambda x:x[-1],False).take(10)\n\n\nreviews_by_prod = filter_movie_data_rdd.map(lambda x:(x[1],x[5])).groupByKey()\n\nreviews_by_prod_sort = reviews_by_prod.mapValues(lambda l : sorted(l,key = lambda x : x)[int(len(l)/2)])\nreviews_by_prod_sort_10 = reviews_by_prod_sort.sortBy(lambda x:x[-1],False).take(10)\n\n\n\nprint(reviews_by_user_sort_10)\n\nprint(reviews_by_prod_sort_10)\nt2 = time.time()\nprint(t2-t1)\n", "execution_count": 104, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "4f16365d9a8e4e38bf2c68ebb3e22f19"}}, "metadata": {}}, {"output_type": "stream", "text": "[(51865782, 440), (25628286, 251), (42072921, 243), (46097534, 228), (37118941, 227), (29580246, 201), (50476169, 200), (21809895, 194), (50595705, 191), (10794201, 185)]\n[('B00LTQ5EVY', 984), ('B00T7TYTCK', 503), ('B009SF2GZU', 321), ('B009SF2IRG', 308), ('B005ZHBBU6', 293), ('B0000020FQ', 270), ('B0002IJNGC', 268), ('B000RY431G', 267), ('B000003G29', 267), ('B00IOQSW7A', 256)]\n1734.204293012619", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "# Stage3. Similarity analysis with Sentence Embedding"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "from pyspark.sql import SparkSession\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport numpy as np\n", "execution_count": 105, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "d2ad0cb5af624645a325451a9954d3e7"}}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "### Stage3 --- (a) pick one of top 10 product"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "t1 = time.time()\nnumber_of_reviews_for_single_product_10 = spark.read.csv(store_route+'number_of_reviews_for_single_product.csv', header=True, inferSchema=True)\npickproduct_id = number_of_reviews_for_single_product_10.collect()[-1][\"product_id\"]\nprint(pickproduct_id)\n# this dataframe won't be used again, so delete it, release memory\ndel number_of_reviews_for_single_product_10", "execution_count": 106, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "3acaed8538f44ae4a9bfdb2faa744617"}}, "metadata": {}}, {"output_type": "stream", "text": "B00006J6VG", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "filter_movie_data_rdd = spark.read.csv(store_route+'filter_movie_data.csv', header=True, inferSchema=True).rdd\n# review_star will be used multiple times so cache it\nreview_star = filter_movie_data_rdd.filter(lambda l : l[0] == pickproduct_id).map(lambda l : (l[1],l[2],l[3])).cache()\n# this dataframe won't be used again, so delete it, release memory\ndel filter_movie_data_rdd", "execution_count": 107, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "7dc7c98796a24dddad6eeba8d207332c"}}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "### get negative and positive reviews cluster\n\n"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "#get negative and positive reviews cluster\nnegative_reviews = review_star.filter(lambda l: l[1]<=2)\npositive_reviews = review_star.filter(lambda l: l[1]>=4)\n# print(negative_reviews.collect())\n", "execution_count": 108, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "59c2f58bbb3d4558a6c96ed6844173d5"}}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "negative_reviews_divide = negative_reviews.flatMap(dividesentences).map(lambda l : (l[0],re.sub(r'</?\\w+[^>]*>','',l[-1]))).cache()\npositive_reviews_divide = positive_reviews.flatMap(dividesentences).map(lambda l : (l[0],re.sub(r'</?\\w+[^>]*>','',l[-1]))).cache()\n", "execution_count": 109, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "f5d604539ab0444c92d1037f0562d949"}}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "from pyspark.sql import Row\n\nnegative_reviews_divide_df = negative_reviews_divide.map(lambda l : Row(review_id= l[0],reviews_body = l[1])).toDF()\npositive_reviews_divide_df = positive_reviews_divide.map(lambda l : Row(review_id= l[0],reviews_body = l[1])).toDF()\n\nnegative_reviews_divide_df.write.csv(store_route+'negative_reviews_divide_df.csv', header = True, mode = \"overwrite\")\npositive_reviews_divide_df.write.csv(store_route+'positive_reviews_divide_df.csv', header = True, mode = \"overwrite\")\n\n\n", "execution_count": 110, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "2039c79788884ac0b58c70e6f0b350f6"}}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def getcenterpoint(reviews_divide,modelname):\n    print(\"===================================Start===================================\")\n\n    print(modelname+\" center cluster : ---------------------------------------------------------\")\n    #get the sentence list \n    text_list = reviews_divide.map(lambda l : l[1]).collect()\n    text_rdd = reviews_divide.map(lambda row: str(row[-1])).filter(lambda data: data is not None).cache()\n    \n    \n    #get the review id list \n    reviewid_list = reviews_divide.map(lambda l : l[0]).collect()\n    \n    review_embedding_list = np.array(review_embed(text_rdd.collect()))\n\n\n    average_distance = []\n    review_embedding_array = np.array(review_embedding_list)\n    review_embedding_array_norm = (review_embedding_array**2).sum(axis=1)**0.5\n\n    #use matrix to calculate distance\n    #convert the original method(1-cos(Theta)) to \n    #avg_distance = len(vector_list)-vector.dot(sum(direction of all vector))\n    for i in range(np.shape(review_embedding_array)[0]):\n        line = np.array([review_embedding_array[i]])\n        line_dis = (np.sum(line**2)**0.5)*review_embedding_array_norm**(-1)\n        dot_value = np.dot(review_embedding_array,line.T)\n        average_dis = (np.shape(review_embedding_array)[0] - np.dot(dot_value.T,line_dis).sum())/np.shape(review_embedding_array)[0]\n        average_distance.append(average_dis)\n        \n    #get the center_id_index -- the minimum \n    center_id_index = average_distance.index(np.min(average_distance))\n    \n    \n    #get the center_id by center_id_index\n    center_id = reviewid_list[center_id_index]\n    \n    \n    #get the center_vec to calculate the distance with other sentence \n    center_vec = np.array([review_embedding_array[center_id_index]])\n    print(\"center review id:\"+str(center_id))\n    print(\"center sentence:\"+str(text_list[center_id_index]))\n\n    cos_dis = np.dot(review_embedding_array,center_vec.T)\n    line_dis = (np.sum(center_vec**2)**0.5)*review_embedding_array_norm\n    center_dis = []\n    for i in range(np.shape(line_dis)[0]):\n        center_dis.append(1-(cos_dis[i][0]/line_dis[i]))\n    \n    \n    #get the index of cloestest 10 sentences  \n    cloest_10 = np.array(center_dis).argsort()[1:11]\n    for i in cloest_10:\n        print(\"review id:\" + str(reviewid_list[i]))\n        print(\"sentence:\"+str(text_list[i]))\n\n    print(\"====================================End====================================\")\n\n\n", "execution_count": 111, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "b4a478d03bfb49f7a58343cebb64d455"}}, "metadata": {}}]}, {"metadata": {"scrolled": true, "trusted": true}, "cell_type": "code", "source": "getcenterpoint(positive_reviews_divide,\"positive\")\ngetcenterpoint(negative_reviews_divide,\"negative\")\nt2 = time.time()\n\nprint(t2-t1)\n", "execution_count": 112, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "eccc6e453f3e4c338a2c2958e73ad36c"}}, "metadata": {}}, {"output_type": "stream", "text": "===================================Start===================================\npositive center cluster : ---------------------------------------------------------\ncenter review id:R1VR2BOC4IT29H\ncenter sentence:Every song is great!\nreview id:R2L30H6HKZXUK7\nsentence:every song is really good.\nreview id:R144NZND4C5S5A\nsentence:Every single song is GREAT!\nreview id:R2N0NUDL7GEOCN\nsentence:i LOVE every single song.\nreview id:R22YSMYT6ZBKWH\nsentence:all the songs are great.\nreview id:RK0YKYW86PK2N\nsentence:all the songs are great.\nreview id:R1ZAV0KB4C8FY5\nsentence:The songs are all great!\nreview id:R32U2AEZWXU5X0\nsentence:Almost all the songs on here are great.\nreview id:R71OTQFQVOIVL\nsentence:The BEST SONG ON THE ALBLUM IS HOLD ON!\nreview id:R2H1YNHUCT31TS\nsentence:It's got a great variation of songs, and every one of them is worth listening to.\nreview id:R1GQBYL3HBOYSZ\nsentence:I love every song in this C.D.\n====================================End====================================\n===================================Start===================================\nnegative center cluster : ---------------------------------------------------------\ncenter review id:R2L4PZC7CHGQ4R\ncenter sentence:This album is much too insipid for people who like to listen to real music.\nreview id:R36BAXRVCR2PFB\nsentence:I bet most people who like this album don't even like real punk.\nreview id:R2UBEJ5JX4PKX1\nsentence:Almost everything that is wrong with mainstream music today, is represented in this album.\nreview id:R195DYX83KWYXU\nsentence:Simply put, this album is mediocre at best.\nreview id:R3QG1GKJO8IL4K\nsentence:This album has in no way let down that genre.\nreview id:R1DP63VJ4NXY44\nsentence:This album absolutely sucks.\nreview id:R1DP63VJ4NXY44\nsentence:This album is full of teeny-bopper pop songs disguised as rock.\nreview id:R1CA8OIEZ0B22Y\nsentence:I do like Blink 182 but most of this type of this music is junk.\nreview id:RKN17VFTZQ69P\nsentence:This album is horrid.\nreview id:RATB9UCW9ZV0B\nsentence:This album is a waste of time and money.\nreview id:RKEOBZVEWY7RW\nsentence:those sad people saying this is 'the greatest album in years' need to listen to some proper music, not watch Mtv all day.\n====================================End====================================\n215.83181166648865", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "# Stage4. Similarity analysis with Spark supported Feature Extractors"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def Pca(X,k=21):\n    '''\n    Pca decompose: reduce the calculation dimension, default dimension is 21\n    '''\n    n_samples, n_features = X.shape\n    mean=np.array([np.mean(X[:,i]) for i in range(n_features)])\n    norm_X=X-mean\n    scatter_matrix=np.dot(np.transpose(norm_X),norm_X)\n    eig_val, eig_vec = np.linalg.eig(scatter_matrix)\n    eig_pairs = [(np.abs(eig_val[i]), eig_vec[:,i]) for i in range(n_features)]\n    eig_pairs.sort(reverse=True)\n    feature=np.array([ele[1] for ele in eig_pairs[:k]])\n    data=np.dot(norm_X,np.transpose(feature))\n    return data\n\ndef Norm(raw_data):\n    '''\n    nomalization : convert the value to 0-1 \n    '''\n    minVals = raw_data.min(0)\n    maxVals = raw_data.max(0)\n\n    ranges = maxVals - minVals\n    normDataSet = np.zeros(np.shape(raw_data))\n    m = raw_data.shape[0]\n    normDataSet = raw_data - np.tile(minVals, (m, 1))\n    normDataSet = normDataSet/np.tile(ranges, (m, 1))\n    return normDataSet", "execution_count": 113, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "2949e7fd91f84980830ca85c3798e26a"}}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "t1 = time.time()\n#get negative_reviews and positive_reviews from S3,these data has been stored in Stage3\nnegative_reviews_divide_df = spark.read.csv(store_route+'negative_reviews_divide_df.csv', header=True, inferSchema=True)\npositive_reviews_divide_df = spark.read.csv(store_route+'positive_reviews_divide_df.csv', header=True, inferSchema=True)\n\n\nnegative_reviews_divide = negative_reviews_divide_df.rdd\npositive_reviews_divide = positive_reviews_divide_df.rdd\n\nnegative_text_rdd = negative_reviews_divide.map(lambda row: str(row[-1])).filter(lambda data: data is not None).cache()\npositive_text_rdd = positive_reviews_divide.map(lambda row: str(row[-1])).filter(lambda data: data is not None).cache()\n\nnegative_text_list = negative_reviews_divide.map(lambda l : l[1]).collect()\npositive_text_list = positive_reviews_divide.map(lambda l : l[1]).collect()\n", "execution_count": 114, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "fb127bbec34245849bf4f2d7745d016f"}}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "### Create training model", "execution_count": 115, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "62288fbe95894cd4b1638a5e82ba811e"}}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "from pyspark.ml.feature import Word2Vec,HashingTF,IDF,Tokenizer\nfrom nltk.tokenize import WordPunctTokenizer\nword_tokenizer = WordPunctTokenizer()\n# pattern = r\"\\w+(?:[-']\\w+)*|'|[-.(]+|\\S\\w*\"\n\n\n#segement the sentence to words\nnegative_word_list = [(word_tokenizer.tokenize(str(sentence)),) for sentence in negative_text_list]\npositive_word_list = [(word_tokenizer.tokenize(str(sentence)),) for sentence in positive_text_list]\n# transform to dataframe\nnegative_documentDF = spark.createDataFrame(negative_word_list, [\"text\"])\npositive_documentDF = spark.createDataFrame(positive_word_list, [\"text\"])\n\n\n\nword2Vec = Word2Vec(vectorSize=50, minCount=0, inputCol=\"text\", outputCol=\"result\")\n\nnegative_model = word2Vec.fit(negative_documentDF)\npositive_model = word2Vec.fit(positive_documentDF)\n\nmodeldict = {\"positive\":positive_model,\"negative\":negative_model}\ndataframedict = {\"positive\":positive_documentDF,\"negative\":negative_documentDF}", "execution_count": 116, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "fc777a0749ee4598b66125ae949ab012"}}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# use trained model to get trained result\ndef spl_word_vec_embedding(modelchoice):\n    current_model = modeldict[modelchoice]\n    dataDF = dataframedict[modelchoice]\n    embeddingresult = current_model.transform(dataDF).select(\"result\").collect()\n    embedding_array =np.array([i[\"result\"] for i in embeddingresult])\n    return embedding_array\n\n\n\n# print(np.shape(a))\n# print(np.shape(b))\n# print(np.shape(c))\n# print(c)", "execution_count": 117, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "76217c5cdf154b80b53a759c1e281d77"}}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def getcenterand_toptenpoint_word2vec(reviews_divide,modelname):\n    print(\"====================================Start====================================\")\n\n    print(modelname+\" center cluster : ---------------------------------------------------------\")\n    text_list = reviews_divide.map(lambda l : l[1]).collect()\n    print\n    text_rdd = reviews_divide.map(lambda row: str(row[-1])).filter(lambda data: data is not None).cache()\n    reviewid_list = reviews_divide.map(lambda l : l[0]).collect()\n\n\n    review_embedding_array = Pca(spl_word_vec_embedding(modelname))\n    average_distance = []\n#     print(np.shape(review_embedding_array))\n    review_embedding_array_norm = np.array([(review_embedding_array**2).sum(axis=1)**0.5]).T\n    \n    \n    #use matrix to calculate distance\n    #convert the original method(1-cos(Theta)) to \n    #avg_distance = len(vector_list)-vector.dot(sum(direction of all vector))\n    for i in range(np.shape(review_embedding_array)[0]):\n        line = np.array([review_embedding_array[i]])\n        line_dis = (np.sum(line**2)**0.5)*review_embedding_array_norm**(-1)\n        dot_value = np.dot(review_embedding_array,line.T)\n        average_dis = (np.shape(review_embedding_array)[0] - (dot_value/line_dis).sum())/np.shape(review_embedding_array)[0]\n        average_distance.append(average_dis)\n        \n    #get the center_id by center_id_index\n    center_id_index = average_distance.index(min(average_distance))\n    center_id = reviewid_list[center_id_index]\n#     print(center_id)\n\n    #get the center_vec to calculate the distance with other sentence \n    center_vec = np.array([review_embedding_array[center_id_index]])\n    print(\"center review id:\"+str(center_id))\n    print(\"center sentence:\"+str(text_list[center_id_index]))\n\n    cos_dis = np.dot(review_embedding_array,center_vec.T)\n    line_dis = (np.sum(center_vec**2)**0.5)*review_embedding_array_norm\n    center_dis = []\n    \n    #get the index of cloestest 10 sentences  \n    for i in range(np.shape(line_dis)[0]):\n        center_dis.append(1-(cos_dis[i]/line_dis[i]))\n    center_dis = [i[0] for i in center_dis]\n    cloest_10 = np.array(center_dis).argsort()[1:11]\n    for i in cloest_10:\n        print(\"review id:\" + str(reviewid_list[i]))\n        print(\"sentence:\"+str(text_list[i]))\n    print(\"====================================End====================================\")\n\n", "execution_count": 118, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "68541e0ded5d4b828b3808c13d4579a7"}}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "getcenterand_toptenpoint_word2vec(positive_reviews_divide,\"positive\")\ngetcenterand_toptenpoint_word2vec(negative_reviews_divide,\"negative\")\n\nt2 = time.time()\n\nprint(t2-t1)\n", "execution_count": 119, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "d5a90a90308c40c18983f591f6cbc4ed"}}, "metadata": {}}, {"output_type": "stream", "text": "====================================Start====================================\npositive center cluster : ---------------------------------------------------------\ncenter review id:R2EKY5I5KJW2PB\ncenter sentence:6.Girls & Boys-5/5-Awesome song!\nreview id:R26OJ99Q1J1B6Q\nsentence:Riot Girl - 5/510.\nreview id:RY3PCDQ80U0O8\nsentence:My Bloody Valentine: 5/5.\nreview id:RY3PCDQ80U0O8\nsentence:Riot Girl: 3/5.\nreview id:R2W54S6JP18GBZ\nsentence:My Bloody Valentine 5/5    8.\nreview id:RY3PCDQ80U0O8\nsentence:Hold On: 5/5.\nreview id:R26OJ99Q1J1B6Q\nsentence:My Bloody Valentine - 4/58.\nreview id:R26OJ99Q1J1B6Q\nsentence:Hold On - 5/59.\nreview id:RPRNHKLZG1OEY\nsentence:Riot Girl~ 4/5 Pretty fast paced.\nreview id:RY3PCDQ80U0O8\nsentence:Girls and Boys: 4/5.\nreview id:R9C2ZW7ODCQRT\nsentence:Riot Girl 4.\n====================================End====================================\n====================================Start====================================\nnegative center cluster : ---------------------------------------------------------\ncenter review id:R1V5HHKRJNWUQM\ncenter sentence:I still can't believe I saw Benji in good charlotte wear a &quot;Rancid radio&quot; shirt.\nreview id:R3H1SO2YOK5GFB\nsentence:I guess I could say &quot;punk is dead&quot;, but really it is all about a mindset that many &quot;punk followers&quot; (yes even you hardcore kids that listen to &quot;underground punk bands&quot; like the germs, mc5 and agnostic front) can't even begin to comprehend - but I'm not saying i can either.\nreview id:R51IS66I169AG\nsentence:I laugh when I see kids turn all &quot;punk&quot; (spike arm bands and ripped pants don't make anyone punk) and only to find out their favourite band is GC.\nreview id:R3RHJFESD45TA4\nsentence:I'm not a &quot;trust fund kid&quot; and i have a good idea &quot;what punk is&quot; and it sure as f**k isn't your poison for the ears.\nreview id:R386QBTULFUGV\nsentence:Come on, some idiot said &quot;if you have so much knowledge of punk then why don't I see your face on my TV everyday?&quot; Well.\nreview id:R3JYNCM627LBF2\nsentence:No one seems to see that the music that they all play isn't &quot;punk&quot;, it's just &quot;rock n roll&quot;.\nreview id:R2AQNU1AWT0HM5\nsentence:I mean if you listen to the track &quot;Girls and Boys&quot; and even look at the video it's a total mess!\nreview id:R1P6AETM3HJIDO\nsentence:I suggest Wakefield's new CD &quot;American Made&quot; Its awesome.\nreview id:R2KN2QAOASMI3U\nsentence:&quot;Boys like girls but girls like cars and money.&quot; That's deep.\nreview id:RJGXY7ZUOB9WB\nsentence:A certain reviewer said &quot;Alright many people dont like them because they are punk&quot;, and &quot;I recommed the Cd for anyone who likes punk.&quot; Honestly, I think that was a typo and that you meant to say &quot;many people don't like them because they are not punk, but claim to be punk.&quot; Anyways, I cringe when I think what else you think is &quot;punk.&quot;Also you said &quot;Dont judge them just because they are popular, doesnt mean their pop.&quot; WTF?\nreview id:RJGXY7ZUOB9WB\nsentence:Don't you know that &quot;pop&quot; is short for &quot;popular&quot;?Congratulations!\n====================================End====================================\n13.9733726978302", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "pysparkkernel", "display_name": "PySpark", "language": ""}, "language_info": {"name": "pyspark", "mimetype": "text/x-python", "codemirror_mode": {"name": "python", "version": 2}, "pygments_lexer": "python2"}}, "nbformat": 4, "nbformat_minor": 2}